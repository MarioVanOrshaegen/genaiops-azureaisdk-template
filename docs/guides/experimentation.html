<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Experimentation Guide - Project Documentation</title>
    <link rel="stylesheet" href="../assets/css/style.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-yaml.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css">
    <style>
        .warning {
            background-color: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 1rem;
            margin: 1rem 0;
        }
        .folder-structure {
            font-family: monospace;
            white-space: pre;
            background-color: #f6f8fa;
            padding: 1rem;
            border-radius: 6px;
            margin: 1rem 0;
        }
    </style>
</head>
<body>
    <header>
        <div class="project-name">Project Name</div>
    </header>

    <div class="sidebar">
        <nav>
            <ul class="nav-list">
                <li><a href="../index.html">Home</a></li>
                <li><a href="getting-started.html">Getting Started</a></li>
                <li><a href="setup-cloud-infra.html">Cloud Infrastructure Setup</a></li>
                <li><a href="local-setup-conda.html">Local Setup</a></li>
                <li><a href="local-setup-uv.html">Local Setup</a></li>
                <li><a href="setup-github-repo.html">GitHub Setup</a></li>
            </ul>
        </nav>
    </div>

    <main class="main-content">
        <h1>Experimentation Configuration Guide</h1>

        <h2>Overview</h2>
        <p>Each use case in the project contains its own experiment configuration and environment settings. This guide explains the structure and configuration of experiments using the <code>math_coding</code> use case as an example.
           The whole purpose of experimentation is to find the best prompt and model configuration. Add multiple prompty files and use them with different model configuration to find the best combination and then use the same before pushing changes as PR to the repo. 
        </p>

        <div class="warning">
            <strong>Important:</strong> The <code>.env</code> file contains sensitive information and should never be committed to the repository. Make sure it's listed in <code>.gitignore</code>.
        </div>

        <h2>Project Structure</h2>
        <div class="folder-structure">
use_cases/
└── math_coding/
    ├── experiment.yaml           # Base configuration
    ├── experiment.dev.yaml       # Development environment overrides
    ├── experiment.prod.yaml      # Production environment overrides
    ├── flows/
    │   └── math_code_generation/
    │       └── pure_python_flow.py
    ├── evaluators/
    │   ├── eval_f1_score.py
    │   └── eval_accuracy.py
    └── data/
        ├── math_data.jsonl
        └── advanced_math_data.jsonl</div>

        <h2>Configuration Files</h2>
        
        <h3>Base Experiment Configuration (experiment.yaml)</h3>
        <pre><code class="language-yaml"># Name of the experiment. Must match use case folder name
name: math_coding

# Description of what the experiment does
description: "This is a math coding experiment"

# Path to the flow definition file
flow: flows/math_code_generation

# Function to be called when flow starts
entry_point: pure_python_flow:get_math_response

# List of available connection configurations
connections:
  - name: aoai
    connection_type: AzureOpenAIConnection
    api_base: https://demoopenaiexamples.openai.azure.com/
    api_version: 2023-07-01-preview
    api_key: ${AOAI_API_KEY}
    api_type: azure
    deployment_name: ${GPT4O_DEPLOYMENT_NAME}
  
  - name: gpt4o
    connection_type: AzureOpenAIConnection
    api_base: https://demoopenaiexamples.openai.azure.com/
    api_version: 2023-07-01-preview
    api_key: ${GPT4O_API_KEY}
    api_type: azure
    deployment_name: ${GPT4O_DEPLOYMENT_NAME}

# Connections to use
connections_ref:
  - aoai
  - gpt4o

# Environment variables
env_vars:
  - PROMPTY_FILE: math_template.prompty
  - MODEL_MAX_TOKENS: "2000"
  - TEMPERATURE: "0.7"

# Evaluation configuration
evaluators:
  - name: eval_f1_score
    flow: evaluations
    entry_point: pure_python_flow:get_math_response
    connections_ref:
      - aoai
      - gpt4o
    env_vars:
      - ENABLE_TELEMETRY: True
    datasets:
      - name: math_coding_test
        source: data/math_data.jsonl
        description: "Basic math problem evaluation dataset"
        mappings:
          ground_truth: "${data.answer}"
          response: "${target.response}"
      - name: advanced_math_test
        source: data/advanced_math_data.jsonl
        description: "Advanced math problem evaluation dataset"
        mappings:
          ground_truth: "${data.answer}"
          response: "${target.response}"

  - name: eval_accuracy
    flow: evaluations
    entry_point: pure_python_flow:evaluate_accuracy
    connections_ref:
      - aoai
    env_vars:
      - ENABLE_TELEMETRY: True
    datasets:
      - name: math_coding_test
        source: data/math_data.jsonl
        description: "Accuracy evaluation dataset"
        mappings:
          ground_truth: "${data.expected}"
          response: "${target.actual}"</code></pre>

        <h3>Environment-Specific Configuration (experiment.dev.yaml)</h3>
        <p>Environment-specific files can override settings from the base configuration:</p>
        <pre><code class="language-yaml"># Override base settings for development
env_vars:
  - TEMPERATURE: "0.9"
  - DEBUG_MODE: True
  - LOG_LEVEL: "DEBUG"

# Use different dataset for development
evaluators:
  - name: eval_f1_score
    datasets:
      - name: math_coding_test_dev
        source: data/math_data_dev.jsonl</code></pre>

        <h2>Components Explanation</h2>

        <h3>Connections</h3>
        <p>Connections define how to interact with external services (like Azure OpenAI). They use environment variables for sensitive information:</p>
        <ul>
            <li><code>name</code>: Unique identifier for the connection</li>
            <li><code>connection_type</code>: Type of service connection</li>
            <li><code>api_base</code>: Service endpoint URL</li>
            <li><code>api_key</code>: Authentication key (from .env)</li>
        </ul>

        <h3>Evaluators</h3>
        <p>Each evaluator represents a different evaluation method:</p>
        <ul>
            <li>Must have a corresponding Python file in the evaluators folder</li>
            <li>Can use multiple datasets for testing</li>
            <li>Can have its own environment variables and connections</li>
        </ul>

        <h3>Datasets</h3>
        <p>Datasets are used for evaluation:</p>
        <ul>
            <li>Located in the data folder</li>
            <li>JSONL format for consistent data handling</li>
            <li>Mappings define how to match ground truth with responses</li>
        </ul>

        <h2>Environment Variables</h2>
        <p>Add necessary secrets and configurations in .env file:</p>
        <pre><code class="language-bash"># Azure OpenAI Configuration
AOAI_API_KEY=your-api-key
GPT4O_DEPLOYMENT_NAME=your-deployment
GPT4O_API_KEY=your-gpt4-key

# Project Configuration
ENABLE_TELEMETRY=True</code></pre>

        <h2>Best Practices</h2>
        <ul>
            <li>Never commit .env files to version control</li>
            <li>Use environment-specific files for different settings</li>
            <li>Keep sensitive information in environment variables</li>
            <li>Use descriptive names for evaluators and datasets</li>
            <li>Document mappings clearly in dataset configurations</li>
            <li>Maintain consistent naming conventions across files</li>
        </ul>

        <h2>Executing Experiments</h2>
        <p>Once you've set up your configuration, you can run experiments using the Python CLI:</p>

        <h3>1. Basic Execution</h3>
        <pre><code class="language-bash"># Run the experiment with dev configuration for math_coding use case
python -m llmops.eval_experiments --environment_name dev --base_path math_coding --report_dir .
        </code></pre>

        <h3>2. Running with different experiment files</h3>
        <pre><code class="language-bash"># Run any_experiment.dev.yaml experiment
python -m llmops.eval_experiments --environment_name dev --base_path math_coding --report_dir . experiment_config_file=any_experiment.dev.yaml
        </code></pre>

        <h3>Monitoring Execution</h3>
        <p>During execution, you'll see:</p>
        <ul>
            <li>Progress indicators for each evaluator</li>
            <li>Evaluation metrics and results</li>
            <li>Any errors or warnings</li>
            <li>Evaluation information is stored in AI Foundry</li>
        </ul>

        <h3>Output and Results</h3>
        <p>Experiment results are typically stored in:</p>
        <ul>
            <li>results file with timestamp-based directories</li>
            <li>Metrics and evaluation results in JSON format</li>
            <li>Detailed logs for debugging and analysis</li>
            <li>Summary reports for quick overview</li>
        </ul>

        <h3>Actions</h3>
        <p>Find:</p>
        <ul>
            <li>best prompty file among all</li>
            <li>best Model configuration including temperature</li>
        </ul>

        <div class="warning">
            <strong>Important:</strong> Always verify that your .env file is properly configured before running experiments. Missing or incorrect environment variables can cause execution failures.
        </div>

        <h2>Adding New Use Cases</h2>
        <p>To add a new use case:</p>
        <ol>
            <li>Create a new top-level folder similar to math_coding </li>
            <li>Copy and modify the experiment configuration structure</li>
            <li>Update evaluators, flows and datasets for your specific needs</li>
            <li>Ensure all required environment variables are documented</li>
        </ol>

        <div class="warning">
            <strong>Note:</strong> Each use case should follow this structure while maintaining its own specific configuration, evaluators, and datasets. The example shown is for math_coding, but the same structure applies to all use cases.
        </div>
    </main>
</body>
</html>